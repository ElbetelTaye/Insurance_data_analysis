{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_22260\\372434951.py:4: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(txt_file_path, delimiter='|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully converted from .txt to C:/Users/elbet/Downloads/10_academy/converted_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elbet\\AppData\\Local\\Temp\\ipykernel_22260\\372434951.py:12: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert .txt file to .csv\n",
    "# Load the .txt file (pipe-delimited)\n",
    "txt_file_path = 'C:/Users/elbet/Downloads/10_academy/Technical Content-week3/Technical Content/Data/MachineLearningRating_v3/MachineLearningRating_v3.txt'\n",
    "df = pd.read_csv(txt_file_path, delimiter='|')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = 'C:/Users/elbet/Downloads/10_academy/converted_data.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(f\"Data successfully converted from .txt to {csv_file_path}\")\n",
    "\n",
    "# Load the newly created CSV file for further analysi\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "tf_device='/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('C:/Users/elbet/Downloads/10_academy/converted_data.csv')\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "# Handling missing data: Impute missing values with median for numerical and mode for categorical features\n",
    "# Apply median imputation only to numeric columns\n",
    "df.fillna(df.select_dtypes(include='number').median(), inplace=True)\n",
    "\n",
    "# For non-numeric columns (e.g., categorical, datetime), (impute using mode) (most frequent value)\n",
    "df.fillna(df.select_dtypes(exclude='number').mode().iloc[0], inplace=True)\n",
    "\n",
    "\n",
    "# Feature Engineering: Create a new feature (e.g., ClaimsRatio: TotalClaims / TotalPremium)\n",
    "df['ClaimsRatio'] = df['TotalClaims'] / df['TotalPremium']\n",
    "\n",
    "\n",
    "# Limit the number of unique categories for high-cardinality features like PostalCode\n",
    "top_n = 100  # Set a threshold for how many top categories to keep\n",
    "df['PostalCode'] = df['PostalCode'].apply(lambda x: x if x in df['PostalCode'].value_counts().index[:top_n] else 'Other')\n",
    "# Encoding categorical data: One-Hot Encoding for categorical variables like Province, Gender, PostalCode\n",
    "df_encoded = pd.get_dummies(df, columns=['Province', 'Gender', 'PostalCode'])\n",
    "\n",
    "# Splitting features and labels\n",
    "X = df_encoded.drop(columns=['TotalPremium', 'TotalClaims'])  # Features\n",
    "y = df_encoded['TotalPremium']  # Target variable: TotalPremium (TotalClaims)\n",
    "\n",
    "# Train-Test Split (80% training and 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Building\n",
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor Model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')  # Appending the path to access the scripts folder\n",
    "from evaluate_fun import * \n",
    "#Step 3: Model Evaluation\n",
    "# Function to evaluate models\n",
    "# Evaluate Linear Regression\n",
    "evaluate_model(lr_model, X_test, y_test, \"Linear Regression\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# ======================================\n",
    "# Step 4: Feature Importance Analysis\n",
    "# ======================================\n",
    "\n",
    "# Feature Importance for Random Forest\n",
    "rf_importances = rf_model.feature_importances_\n",
    "rf_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_importances})\n",
    "rf_importance_df = rf_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nRandom Forest Feature Importances:\\n\", rf_importance_df)\n",
    "\n",
    "# Feature Importance for XGBoost\n",
    "xgb_importances = xgb_model.feature_importances_\n",
    "xgb_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_importances})\n",
    "xgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nXGBoost Feature Importances:\\n\", xgb_importance_df)\n",
    "\n",
    "# Plotting Feature Importance for XGBoost\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "# Step 5: Model Interpretability using SHAP\n",
    "# ======================================``\n",
    "\n",
    "# Initialize SHAP for XGBoost\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names=X_test.columns)\n",
    "\n",
    "# SHAP Dependence Plot for a specific feature (example: 'ClaimsRatio')\n",
    "shap.dependence_plot('ClaimsRatio', shap_values, X_test, feature_names=X_test.columns)\n",
    "\n",
    "# SHAP Force Plot for a single instance\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], feature_names=X_test.columns)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
